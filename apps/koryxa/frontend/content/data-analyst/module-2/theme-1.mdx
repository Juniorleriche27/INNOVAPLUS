---
title: "Thème 1 — Panorama des sources & plan de collecte"
module: "Module 2 — Collecte des données"
estimated_reading_time: "70–100 min"
---

# 1) Pourquoi la collecte est un vrai travail (et pas “juste récupérer un fichier”)

Quand une analyse sort de mauvais résultats, on accuse souvent :
- “les données sont nulles”,
- “l’outil bug”,
- “le dashboard ne reflète pas la réalité”.

Dans la majorité des cas, la cause est plus simple :
**la collecte a été faite sans méthode**.

Une collecte mal cadrée produit 4 problèmes classiques :

1) **Tu récupères des données inutiles**
   - Beaucoup de colonnes, beaucoup de tables… mais aucune n’aide une décision.
   - Résultat : tu passes du temps à nettoyer des infos qui ne serviront jamais.

2) **Tu mélanges des niveaux de détail (grain)**
   - Exemple : une table “commandes” (1 ligne = 1 commande) avec une table “articles” (1 ligne = 1 article).
   - Si tu joins mal, tu doubles/triples les montants et tu crées des KPI faux.

3) **Tu perds la traçabilité**
   - Personne ne sait : “d’où vient ce chiffre ?”
   - Ni quand l’extraction a été faite, ni quel filtre a été appliqué, ni quelle version.

4) **Tu découvres les contraintes trop tard**
   - Accès refusé, API limitée, fichiers incomplets, historique absent…
   - Tu le découvres après avoir promis des résultats.

Objectif de ce thème :
1) identifier toutes les sources possibles (même “hypothétiques” au départ),
2) évaluer vite ce qui est utile et accessible,
3) produire un **inventaire de sources**, un **data mapping minimal**, et un **plan de collecte**,
4) rendre ça reproductible via notebook + exports.

---

# 2) Types de données : prévoir l’effort avant de commencer

## 2.1 Données structurées
Exemples :
- tables SQL (PostgreSQL, MySQL…)
- CSV “propres”
- tableaux simples (commandes, inscriptions, paiements)

Points forts :
- jointures claires, agrégations faciles
- contrôles simples (doublons, clés, types)

Risques :
- clés manquantes
- grain incohérent
- colonnes dupliquées (mêmes infos dans plusieurs champs)

## 2.2 Données semi-structurées
Exemples :
- JSON d’API
- logs d’événements (event_type, event_time, metadata)

Points forts :
- très riches, adaptables
- parfaites pour analyser des parcours (funnels)

Risques :
- schéma qui change
- champs imbriqués (nested)
- pagination / limites API

## 2.3 Données non structurées
Exemples :
- tickets support, texte libre
- PDF, images

Points forts :
- beaucoup de contexte métier

Risques :
- plus coûteux à exploiter
- souvent hors-scope au début (on les garde en “phase 2”)

---

# 3) Grandes familles de sources (terrain Data Analyst)

## 3.1 Fichiers (CSV/Excel)
Quand ça arrive :
- exports manuels CRM
- rapports internes
- listes apprenants / clients

Questions à poser tout de suite :
- Combien de fichiers et à quelle fréquence ?
- Format de dates ? séparateur CSV ? encodage ?
- Excel multi-feuilles ? tables “fusionnées” ? lignes de total ?

## 3.2 Bases SQL
Quand ça arrive :
- application (inscriptions, commandes)
- ERP/CRM back-office

Questions à poser :
- qui donne l’accès ?
- quelles tables ? quelles clés ?
- le grain exact ? (1 ligne = 1 commande ? 1 item ? 1 transaction ?)
- historique complet ou partiel ?

## 3.3 APIs
Quand ça arrive :
- paiement, SMS, analytics, outils externes
- open data

Questions à poser :
- auth (API key / OAuth) ?
- pagination (page/limit, cursor) ?
- rate limits ?
- endpoints stables ?
- format (JSON, champs requis) ?

## 3.4 Outils métiers (CRM/ERP, Google Analytics, etc.)
Souvent : export CSV, connecteurs, définitions propres à l’outil.

Questions à poser :
- définitions internes (“utilisateur actif”, “conversion”)
- segmentation disponible ?
- fenêtre d’historique ?

## 3.5 Logs & tracking événements
Exemples :
- opened_theme, opened_notebook, submitted, validated

Questions à poser :
- l’événement est-il loggé partout ?
- timezone ?
- événements manquants ?
- quelles propriétés utiles (country, device, channel) ?

---

# 4) La méthode “Source → KPI → Décision” (la plus simple et la plus efficace)

Avant de collecter, tu dois pouvoir relier :

**Décision → Objectif → KPI → Questions → Données → Sources**

Si une donnée ne se relie à aucune question ou KPI :
- elle devient du bruit,
- elle te ralentit,
- elle augmente les risques (privacy, qualité, stockage).

Exemple rapide :
- Décision : “quelles leçons raccourcir / enrichir ?”
- KPI : completion_rate_m1
- Question : “à quel thème les gens décrochent ?”
- Données : events (opened_theme, validated, timestamps)
- Sources : platform_events + validations

---

# 5) Inventaire des sources (Data Source Inventory)

L’inventaire est un tableau standard qui liste toutes les sources candidates.

## 5.1 Colonnes recommandées (version KORYXA)
- source_name : nom court (slug)
- type : file / sql / api / tool / log
- owner : qui “possède” la source (équipe/role)
- access : comment on récupère (sql, api, export manuel…)
- refresh : daily/weekly/manual
- grain : “1 ligne = quoi”
- key_fields : clés (ex: user_id, order_id)
- coverage : période couverte
- known_issues : problèmes connus (doublons, trous…)
- privacy : low/medium/high (sensibilité)
- linked_kpis : KPI servis

## 5.2 Exemple (bon niveau)
- platform_events (log) : 1 ligne = 1 événement, clé user_id, refresh daily
- validations (sql) : 1 ligne = 1 validation, clé user_id
- marketing (tool) : 1 ligne = 1 user, clé user_id, canal d’acquisition

---

# 6) Data mapping minimal (sans complexité)

Le data mapping te dit :
- quelles sources se relient,
- par quelles clés,
- et où sont les trous.

## 6.1 Exemple simple (KORYXA School)
Sources :
- platform_events (user_id)
- validations (user_id)
- profile (user_id, country)
- marketing (user_id, channel)
- support_tickets (ticket_id, user_id)

Jointures proposées :
- platform_events.user_id -> validations.user_id
- platform_events.user_id -> profile.user_id
- platform_events.user_id -> marketing.user_id
- platform_events.user_id -> support_tickets.user_id

## 6.2 Contrôles indispensables avant d’intégrer
- la clé existe-t-elle dans chaque source ?
- même type (string vs int) ?
- unicité attendue ?
- % de valeurs manquantes sur la clé ?
- volume : combien de user_id “perdus” après jointure ?

---

# 7) Plan de collecte (document opérationnel)

Le plan de collecte répond clairement :
- Quoi collecter ?
- D’où ?
- Comment ?
- Quand ?
- Dans quel ordre ?
- Comment valider la collecte ?

## 7.1 Template (copiable)
1) Contexte & objectif  
2) Sources (inventaire)  
3) Données requises (data requirements)  
4) Méthode d’accès par source (file/sql/api)  
5) Ordre d’extraction (priorités)  
6) Contrôles qualité rapides (tests)  
7) Stockage + organisation des fichiers  
8) Traçabilité (date, versioning, notes)  
9) Risques & mitigation  

## 7.2 Ordre recommandé (pro)
- priorité 1 : sources cœur KPI (events, commandes, validations)
- priorité 2 : segmentation (profile, marketing, device)
- priorité 3 : garde-fous / qualité (support tickets, remboursements)

---

# 8) Exemple complet (KORYXA School — collecte “pro”)

Objectif :
Augmenter completion_rate_m1.

KPI :
- completion_rate_m1 (principal)
- support_ticket_rate (guardrail)
- notebook_opened_48h_rate (leading)

Sources candidates :
- platform_events (log/sql)
- validations (sql)
- support_tickets (api/tool)
- profile (sql/tool)
- marketing (tool)

Plan :
1) confirmer accès owners (KORYXA/Support/Marketing)
2) extraire platform_events + validations (priorité 1)
3) contrôles qualité express :
   - missing sur user_id, event_time
   - doublons
   - min/max dates
   - distribution event_type
4) extraire profile + marketing (segmentation)
5) extraire support_tickets (guardrail)
6) versionner les extractions :
   - dossier daté : `extracts/2026-01-13/`
   - README extraction : filtres, limites
7) produire dataset final + mapping + plan + QC JSON

---

# 9) Contrôles qualité “express” (obligatoires)

Avant toute analyse :
- % manquants par champ
- doublons (au niveau attendu)
- cohérence des dates (min/max, timezone si connue)
- valeurs inattendues (event_type inconnu)
- outliers simples (durées négatives, montants < 0)
- volumes par jour/semaine (trous de tracking)

---

# 10) Exercices (avant quiz)

A) Remplir un inventaire de **8 sources minimum** (même si certaines sont “hypothétiques”)  
B) Écrire un data mapping minimal (au moins 4 sources reliées)  
C) Écrire un plan de collecte 1 page  
D) Exécuter le notebook et générer les exports obligatoires  

---

# 11) Checklist de validation (Thème 1)

- [ ] inventory >= 8 sources
- [ ] data mapping : au moins 4 sources + clés + jointures proposées
- [ ] plan de collecte : étapes + contrôles + stockage + traçabilité
- [ ] exports notebook générés
- [ ] soumission validée
