{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 2 — Thème 2 (CSV/Excel + Power Query)\n",
        "\n",
        "## Objectif\n",
        "À partir de fichiers bruts volontairement \"sales\" :\n",
        "- produire un dataset propre **exactement** avec ces 6 colonnes : `user_id,event_time,event_type,theme,country,channel`\n",
        "- générer un rapport qualité JSON\n",
        "- générer un data dictionary (MD)\n",
        "\n",
        "## Fichiers attendus (inputs)\n",
        "- `raw_events_messy.csv`\n",
        "- `raw_profiles_messy.xlsx`\n",
        "\n",
        "## Fichiers à produire (outputs)\n",
        "- `m2t2_clean_learning_dataset.csv`\n",
        "- `m2t2_quality_report.json`\n",
        "- `m2t2_data_dictionary.md`\n",
        "\n",
        "## Important (Power Query)\n",
        "- Crée aussi un fichier `m2t2_powerquery.m` et colle ton script M (Advanced Editor).\n",
        "- Crée `m2t2_refresh_notes.md` (5–10 lignes) : comment refresh + risques + locale choisie.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"✅ Imports OK\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------- Robust CSV reader (separator + encoding) ----------\n",
        "def read_csv_robust(path: str) -> pd.DataFrame:\n",
        "    attempts = [\n",
        "        {\"sep\": \";\", \"encoding\": \"utf-8\"},\n",
        "        {\"sep\": \",\", \"encoding\": \"utf-8\"},\n",
        "        {\"sep\": \";\", \"encoding\": \"latin1\"},\n",
        "        {\"sep\": \",\", \"encoding\": \"latin1\"},\n",
        "    ]\n",
        "    last_err = None\n",
        "    for a in attempts:\n",
        "        try:\n",
        "            df = pd.read_csv(path, **a)\n",
        "            if df.shape[1] >= 3:\n",
        "                return df\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "    raise last_err\n",
        "\n",
        "events = read_csv_robust(\"raw_events_messy.csv\")\n",
        "profiles = pd.read_excel(\"raw_profiles_messy.xlsx\", sheet_name=0)\n",
        "print(\"Loaded:\", events.shape, profiles.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------- Standardize column names ----------\n",
        "events.columns = [str(c).strip().lower() for c in events.columns]\n",
        "profiles.columns = [str(c).strip().lower() for c in profiles.columns]\n",
        "\n",
        "# tolerant aliasing if needed\n",
        "aliases = {\n",
        "    \"userid\": \"user_id\",\n",
        "    \"user\": \"user_id\",\n",
        "    \"time\": \"event_time\",\n",
        "    \"timestamp\": \"event_time\",\n",
        "    \"event\": \"event_type\",\n",
        "}\n",
        "for k, v in aliases.items():\n",
        "    if k in events.columns and v not in events.columns:\n",
        "        events.rename(columns={k: v}, inplace=True)\n",
        "\n",
        "print(\"Events columns:\", list(events.columns))\n",
        "print(\"Profiles columns:\", list(profiles.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------- Cleaning helpers ----------\n",
        "def clean_str(s: pd.Series) -> pd.Series:\n",
        "    s = s.astype(str)\n",
        "    s = s.str.replace(\"\\u00A0\", \" \", regex=False)  # non-breaking space\n",
        "    s = s.str.strip()\n",
        "    s = s.str.replace(r\"\\s+\", \" \", regex=True)\n",
        "    s = s.replace(\"nan\", \"\")\n",
        "    return s\n",
        "\n",
        "def normalize_country(s: pd.Series) -> pd.Series:\n",
        "    s = clean_str(s)\n",
        "    s = s.str.title()\n",
        "    s = s.replace({\"Benin\": \"Bénin\"})\n",
        "    return s\n",
        "\n",
        "def normalize_channel(s: pd.Series) -> pd.Series:\n",
        "    s = clean_str(s).str.lower()\n",
        "    s = s.replace({\"cellulaire\": \"mobile\", \"smartphone\": \"mobile\"})\n",
        "    return s\n",
        "\n",
        "def normalize_event_type(s: pd.Series) -> pd.Series:\n",
        "    s = clean_str(s).str.lower()\n",
        "    s = s.str.replace(\" \", \"_\", regex=False)\n",
        "    s = s.str.replace(\"-\", \"_\", regex=False)\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------- Ensure expected columns exist ----------\n",
        "for col in [\"user_id\", \"event_time\", \"event_type\", \"theme\", \"country\", \"channel\"]:\n",
        "    if col not in events.columns:\n",
        "        events[col] = \"\"\n",
        "\n",
        "# Apply cleaning\n",
        "events[\"user_id\"] = clean_str(events[\"user_id\"])\n",
        "events[\"event_type\"] = normalize_event_type(events[\"event_type\"])\n",
        "events[\"country\"] = normalize_country(events[\"country\"])\n",
        "events[\"channel\"] = normalize_channel(events[\"channel\"])\n",
        "\n",
        "# theme: coerce numeric\n",
        "events[\"theme\"] = pd.to_numeric(events[\"theme\"], errors=\"coerce\").astype(\"Int64\")\n",
        "\n",
        "# event_time: robust parse (dayfirst common in FR)\n",
        "events[\"event_time\"] = pd.to_datetime(events[\"event_time\"], errors=\"coerce\", dayfirst=True, utc=True)\n",
        "\n",
        "events.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------- Enrich from profiles (optional) ----------\n",
        "if \"user_id\" in profiles.columns:\n",
        "    profiles[\"user_id\"] = clean_str(profiles[\"user_id\"])\n",
        "    if \"country\" in profiles.columns:\n",
        "        profiles[\"country\"] = normalize_country(profiles[\"country\"])\n",
        "    if \"channel\" in profiles.columns:\n",
        "        profiles[\"channel\"] = normalize_channel(profiles[\"channel\"])\n",
        "\n",
        "    keep_cols = [c for c in [\"user_id\", \"country\", \"channel\"] if c in profiles.columns]\n",
        "    prof_u = profiles[keep_cols].drop_duplicates(\"user_id\")\n",
        "    events = events.merge(prof_u, on=\"user_id\", how=\"left\", suffixes=(\"\", \"_profile\"))\n",
        "\n",
        "    if \"country_profile\" in events.columns:\n",
        "        events[\"country\"] = events[\"country\"].mask(events[\"country\"].eq(\"\") | events[\"country\"].isna(), events[\"country_profile\"])\n",
        "    if \"channel_profile\" in events.columns:\n",
        "        events[\"channel\"] = events[\"channel\"].mask(events[\"channel\"].eq(\"\") | events[\"channel\"].isna(), events[\"channel_profile\"])\n",
        "\n",
        "    events.drop(columns=[c for c in [\"country_profile\", \"channel_profile\"] if c in events.columns], inplace=True)\n",
        "\n",
        "print(\"After enrich:\", events.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------- Quality checks ----------\n",
        "expected_cols = [\"user_id\", \"event_time\", \"event_type\", \"theme\", \"country\", \"channel\"]\n",
        "\n",
        "report = {\"created_at\": datetime.utcnow().isoformat() + \"Z\", \"checks\": {}}\n",
        "report[\"checks\"][\"rows_after_load\"] = int(len(events))\n",
        "report[\"checks\"][\"has_expected_columns\"] = bool(all(c in events.columns for c in expected_cols))\n",
        "\n",
        "for c in expected_cols:\n",
        "    report[\"checks\"][f\"missing_{c}\"] = int(events[c].isna().sum()) if c in events.columns else None\n",
        "\n",
        "report[\"checks\"][\"duplicate_rows\"] = int(events[expected_cols].duplicated().sum())\n",
        "\n",
        "dt_min = events[\"event_time\"].min()\n",
        "dt_max = events[\"event_time\"].max()\n",
        "report[\"checks\"][\"event_time_min\"] = None if pd.isna(dt_min) else dt_min.isoformat()\n",
        "report[\"checks\"][\"event_time_max\"] = None if pd.isna(dt_max) else dt_max.isoformat()\n",
        "\n",
        "report[\"checks\"][\"event_type_top10\"] = events[\"event_type\"].value_counts(dropna=False).head(10).to_dict()\n",
        "\n",
        "report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------- Build clean dataset (exact schema) ----------\n",
        "clean = events[expected_cols].copy()\n",
        "clean = clean.dropna(subset=[\"user_id\", \"event_time\", \"event_type\"])  # minimal integrity\n",
        "clean = clean[clean[\"user_id\"].astype(str).str.len() > 0]\n",
        "\n",
        "clean[\"user_id\"] = clean[\"user_id\"].astype(str)\n",
        "clean[\"event_type\"] = clean[\"event_type\"].astype(str)\n",
        "clean[\"country\"] = clean[\"country\"].astype(str)\n",
        "clean[\"channel\"] = clean[\"channel\"].astype(str)\n",
        "\n",
        "clean.to_csv(\"m2t2_clean_learning_dataset.csv\", index=False)\n",
        "print(\"✅ Exported m2t2_clean_learning_dataset.csv\", clean.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------- Export quality report JSON ----------\n",
        "with open(\"m2t2_quality_report.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(report, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"✅ Exported m2t2_quality_report.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------- Data dictionary markdown ----------\n",
        "dict_lines = []\n",
        "dict_lines.append(\"# Data Dictionary — Module 2 / Theme 2\\n\\n\")\n",
        "dict_lines.append(\"## Schéma final (m2t2_clean_learning_dataset.csv)\\n\")\n",
        "dict_lines.append(\"- **user_id**: identifiant apprenant (string)\\n\")\n",
        "dict_lines.append(\"- **event_time**: date/heure événement (UTC, ISO)\\n\")\n",
        "dict_lines.append(\"- **event_type**: type normalisé (ex: enrolled, opened_theme, opened_notebook, submitted, validated)\\n\")\n",
        "dict_lines.append(\"- **theme**: numéro thème (int)\\n\")\n",
        "dict_lines.append(\"- **country**: pays standardisé (Title Case)\\n\")\n",
        "dict_lines.append(\"- **channel**: canal standardisé (lowercase)\\n\\n\")\n",
        "dict_lines.append(\"## Notes qualité (extrait)\\n\")\n",
        "dict_lines.append(f\"- rows_after_load: {report['checks'].get('rows_after_load')}\\n\")\n",
        "dict_lines.append(f\"- duplicate_rows: {report['checks'].get('duplicate_rows')}\\n\")\n",
        "dict_lines.append(f\"- event_time_min: {report['checks'].get('event_time_min')}\\n\")\n",
        "dict_lines.append(f\"- event_time_max: {report['checks'].get('event_time_max')}\\n\")\n",
        "\n",
        "with open(\"m2t2_data_dictionary.md\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.writelines(dict_lines)\n",
        "\n",
        "print(\"✅ Exported m2t2_data_dictionary.md\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------- Power Query M script presence check ----------\n",
        "m_path = \"m2t2_powerquery.m\"\n",
        "if not os.path.exists(m_path):\n",
        "    print(\"⚠️ m2t2_powerquery.m absent. Crée le fichier et colle ton script M depuis Power Query (Advanced Editor).\")\n",
        "else:\n",
        "    content = open(m_path, \"r\", encoding=\"utf-8\", errors=\"ignore\").read().strip()\n",
        "    if len(content) < 30:\n",
        "        print(\"⚠️ m2t2_powerquery.m présent mais trop court/vide. Colle le script M complet.\")\n",
        "    else:\n",
        "        print(\"✅ m2t2_powerquery.m OK (non vide).\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
    "language_info": { "name": "python", "version": "3.x" }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

